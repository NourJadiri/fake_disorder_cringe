{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first approach for Mistrale (Let's Hope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Do the login to hugging face and get the acess Tokin in order to get the model of mistral\n",
    "- !huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKIP THESE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'QuantizationConfig' from 'transformers' (/home/ysidhom/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel, QuantizationConfig\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Determine if CUDA (GPU) is available; if not, default to CPU\u001b[39;00m\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'QuantizationConfig' from 'transformers' (/home/ysidhom/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "\n",
    "# Determine if CUDA (GPU) is available; if not, default to CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for loading the model in 4-bit precision to save memory\n",
    "bitsquant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Load model in 4-bit precision\n",
    "    bnb_4bit_use_double_quant=True,       # Use double quantization for better compression\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Specify quantization type as NF4 (a specific format for quantization)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 as the compute data type for operations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf1ea2598b948a885132fb73b70d4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061f5332d9de4c42adc0f7e68826a96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a7c6ddb8c148e5bd070ae3cf518013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b26656a3c0436784f4a920f4a0184f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00008.bin:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ba3ca4a6d54ac192c048e1359dd067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00008.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1d9aeeda014a3f935d48649bea3404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443fe57c0b0e44e98a924433a4ed6bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00008.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c19e3cd83c4c8985848c82e08b62e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f4e0a8287c44018042dae190ec6a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00008.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e598c6398904d978410b6736dc3399a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00007-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7b00ff3c9e4458ad61b45806d17c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00008-of-00008.bin:   0%|          | 0.00/816M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4766566c7d49589089bbb60b02359c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7835efb6f48049b8b6de1922ddbdb424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Configuration for loading the model in 4-bit precision to save memory\n",
    "bitsquant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Load model in 4-bit precision\n",
    "    bnb_4bit_use_double_quant=True,       # Use double quantization for better compression\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Specify quantization type as NF4 (a specific format for quantization)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 as the compute data type for operations\n",
    ")\n",
    "\n",
    "# mistral ai repository for Mistral-7B with the original tuned weights\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "sharded_id = \"filipealmeida/Mistral-7B-v0.1-sharded\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    sharded_id,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bitsquant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./saved_model\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer to a directory\n",
    "save_path = \"./saved_model\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_6134/3201874984.py:17: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  mistral_llm = HuggingFacePipeline(pipeline=text_gen_pipeline)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "text_gen_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.85, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    pad_token_id=tokenizer.eos_token_id, \n",
    "    repetition_penalty=1.1, \n",
    "    return_full_text=True,\n",
    "    max_new_tokens=256,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_gen_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ysidhom/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.85` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is the usual composition of rocket fuel?\\n\\nThe most common propellants are solid, liquid and hybrid. Solid propellant is a mixture of oxidizer and fuel that is cast into a solid shape. Liquid propellant is a combination of oxidizer and fuel that is stored in separate tanks and mixed just before use. Hybrid propellant is a mixture of oxidizer and fuel that is stored separately but ignited together.\\n\\nWhat is the difference between a rocket and a missile?\\n\\nA rocket is an unguided projectile that flies through space. A missile is a guided projectile that flies through space.\\n\\nHow do you make a homemade rocket?\\n\\nYou can make a homemade rocket by using a soda bottle, a balloon, some tape, and a few other materials. First, cut off the top of the soda bottle and remove the label. Then, blow up the balloon and tie it to the neck of the bottle. Next, tape the balloon to the bottom of the bottle. Finally, add water to the bottle until it is about half full. When you are ready to launch your rocket, simply release the balloon and watch it fly!\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What is the usual composition of rocket fuel?\"\n",
    "mistral_llm.invoke(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyze the following Reddit post and provide concise answers to these features. \n",
      "Use \"Yes\" or \"No\" for binary questions, and specify \"Null\" if information is unclear or not mentioned. Avoid lengthy explanations.\n",
      "\n",
      "Features to extract:\n",
      "\n",
      "Sentiment: Is the sentiment positive, negative, or neutral?\n",
      "Topic: Identify the main topic(s) of the post (be specific and avoid general terms like \"ADHD\").\n",
      "Personal Experience Shared: Does the post mention a personal experience?\n",
      "Mention of Solutions: Are any solutions, advice, or recommendations discussed?\n",
      "Self-Diagnosis: Is there any indication of self-diagnosis?\n",
      "Gender of the Author: Can the gender of the author be inferred?\n",
      "Self-Medication: Is there any mention of self-medication?\n",
      "Post for analysis:\n",
      "Diagnosed with Inattentive ADHD at 31. Explains so many things from my childhood. Please go out and get tested if you are still on the fence. I always assumed ADHD was only hyperactive. A lot of concerns about day dreaming, zoning out and inattentiveness came into play during my consult. I didn't even consider my lack of sleep being tied to ADHD. But now that I have a diagnosis, it explains quite a bit from my past. I wasn't just lazy and disorganized. Again, please go get tested if you suspect anything.\n",
      "\n",
      "Provide the answers in a structured format, directly listing the extracted features..\n"
     ]
    }
   ],
   "source": [
    "text='''\n",
    "Analyze the following Reddit post and provide concise answers to these features. \n",
    "Use \"Yes\" or \"No\" for binary questions, and specify \"Null\" if information is unclear or not mentioned. Avoid lengthy explanations.\n",
    "\n",
    "Features to extract:\n",
    "\n",
    "Sentiment: Is the sentiment positive, negative, or neutral?\n",
    "Topic: Identify the main topic(s) of the post (be specific and avoid general terms like \"ADHD\").\n",
    "Personal Experience Shared: Does the post mention a personal experience?\n",
    "Mention of Solutions: Are any solutions, advice, or recommendations discussed?\n",
    "Self-Diagnosis: Is there any indication of self-diagnosis?\n",
    "Gender of the Author: Can the gender of the author be inferred?\n",
    "Self-Medication: Is there any mention of self-medication?\n",
    "Post for analysis:\n",
    "Diagnosed with Inattentive ADHD at 31. Explains so many things from my childhood. Please go out and get tested if you are still on the fence. I always assumed ADHD was only hyperactive. A lot of concerns about day dreaming, zoning out and inattentiveness came into play during my consult. I didn't even consider my lack of sleep being tied to ADHD. But now that I have a diagnosis, it explains quite a bit from my past. I wasn't just lazy and disorganized. Again, please go get tested if you suspect anything.\n",
    "\n",
    "Provide the answers in a structured format, directly listing the extracted features..'''\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ysidhom/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.85` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "outputs=mistral_llm.invoke(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyze the following Reddit post and provide concise answers to these features. \n",
      "Use \"Yes\" or \"No\" for binary questions, and specify \"Null\" if information is unclear or not mentioned. Avoid lengthy explanations.\n",
      "\n",
      "Features to extract:\n",
      "\n",
      "Sentiment: Is the sentiment positive, negative, or neutral?\n",
      "Topic: Identify the main topic(s) of the post (be specific and avoid general terms like \"ADHD\").\n",
      "Personal Experience Shared: Does the post mention a personal experience?\n",
      "Mention of Solutions: Are any solutions, advice, or recommendations discussed?\n",
      "Self-Diagnosis: Is there any indication of self-diagnosis?\n",
      "Gender of the Author: Can the gender of the author be inferred?\n",
      "Self-Medication: Is there any mention of self-medication?\n",
      "Post for analysis:\n",
      "Diagnosed with Inattentive ADHD at 31. Explains so many things from my childhood. Please go out and get tested if you are still on the fence. I always assumed ADHD was only hyperactive. A lot of concerns about day dreaming, zoning out and inattentiveness came into play during my consult. I didn't even consider my lack of sleep being tied to ADHD. But now that I have a diagnosis, it explains quite a bit from my past. I wasn't just lazy and disorganized. Again, please go get tested if you suspect anything.\n",
      "\n",
      "Provide the answers in a structured format, directly listing the extracted features..\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def get_llm(text_prompt: str) -> str:\n",
    "    return mistral_llm.invoke(text_prompt)\n",
    "\n",
    "# Define the column width for text wrapping \n",
    "# (Medium's typical width is around 80 characters)\n",
    "COLUMN_WIDTH: int = 76\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Remove newline characters and any other special characters\n",
    "    cleaned_text = text.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def wrap_text(text: str, width: int) -> str:\n",
    "    # Use textwrap to wrap text to the specified width\n",
    "    wrapped_text = textwrap.fill(text, width=width)\n",
    "    return wrapped_text\n",
    "\n",
    "def wrap_text_with_comments(text: str, width: int) -> str:\n",
    "    # First wrap the text to specified width, which is < 80 char\n",
    "    # by which we create out text lines\n",
    "    wrapped_text = textwrap.fill(text, width=width)\n",
    "\n",
    "    # Split the wrapped text\n",
    "    lines = wrapped_text.split('\\n')\n",
    "\n",
    "    # Add # character for each line and join the lines\n",
    "    commented_lines = ['# ' + line for line in lines]\n",
    "    commented_text = '\\n'.join(commented_lines)\n",
    "    return commented_text\n",
    "\n",
    "def get_llm_response(text_prompt: str) -> str:\n",
    "    # Get the response from the model\n",
    "    #response = mistral_llm.invoke(text_prompt)\n",
    "    # Clean the output text\n",
    "    cleaned_response = clean_text(text_prompt)\n",
    "    # Wrap the cleaned response text to the specified column width\n",
    "    wrapped_response = wrap_text_with_comments(cleaned_response, COLUMN_WIDTH)\n",
    "    return wrapped_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Analyze the following Reddit post and extract the following features: -\n",
      "# Sentiment: Is the sentiment of the post positive, negative, or neutral? -\n",
      "# Topic: What are the main topic discussed in the post (do not put adhd I want\n",
      "# someting more specific? - Personal experience shared? (Yes/No): - Mention of\n",
      "# solutions? (Yes/No): -self diagnosis? -gendre of person? -self mediaction?\n",
      "# â†’ If there is no clear mention put null and answer only in yes no to\n",
      "# explanation required Post: Diagnosed with Inattentive ADHD at 31. Explains\n",
      "# so many things from my childhood. Please go out and get tested if you are\n",
      "# still on the fence. I always assumed ADHD was only hyperactive. A lot of\n",
      "# concerns about day dreaming, zoning out and inattentiveness came into play\n",
      "# during my consult. I didn't even consider my lack of sleep being tied to\n",
      "# ADHD. But now that I have a diagnoses, it explains quite a bit from my past.\n",
      "# I wasn't just lazy and disorganized. Again, please go get tested if you\n",
      "# suspect anything. It can be life changing.\n"
     ]
    }
   ],
   "source": [
    "clean_response = get_llm_response(outputs)\n",
    "print(clean_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_llm_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clean_response \u001b[38;5;241m=\u001b[39m get_llm_response(text)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(clean_response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_llm_response' is not defined"
     ]
    }
   ],
   "source": [
    "clean_response = get_llm_response(text)\n",
    "print(clean_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start To Focus Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "\n",
    "\n",
    "# Replace with your Hugging Face API token\n",
    "HUGGING_FACE_API_TOKEN = os.getenv(\"Mistrale_Token\")\n",
    "\n",
    "# The API endpoint for the Mistral model\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "#Limit to 1000 requests per day hence 1000 document per day \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting date: 2024-12-18 00:00:00\n",
      "Date in 10 days: 2024-12-28 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Assume x is your starting date\n",
    "x = datetime(2024, 12, 18)  # Example date: December 18, 2024\n",
    "\n",
    "# Add 10 days to x\n",
    "date_in_10_days = x + timedelta(days=10)\n",
    "\n",
    "print(\"Starting date:\", x)\n",
    "print(\"Date in 10 days:\", date_in_10_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your input prompt\n",
    "prompt = '''\n",
    "Analyze the following Reddit post and provide concise answers to these features. \n",
    "Use \"Yes\" or \"No\" for binary questions, and specify \"Null\" if information is unclear or not mentioned. Avoid explanations.\n",
    "\n",
    "Features to extract:\n",
    "\n",
    "Sentiment: positive/negative/neutral\n",
    "Topic: Technology/medication/education/social \n",
    "Personal Experience Shared: Yes/No\n",
    "Mention of Solutions: Yes/No (if any solutions, advice, or recommendations are discussed just answer no explanation)\n",
    "Gender of the Author: Male/Female/Null\n",
    "Self-Diagnosis: Yes/No (if there is any indication of self-diagnosis)\n",
    "Self-Medication: Yes/No\n",
    "Post for analysis:\n",
    "32F Diagnosed with Inattentive ADHD at 31. Explains so many things from my childhood. Please go out and get tested if you are still on the fence. I always assumed ADHD was only hyperactive. A lot of concerns about day dreaming, zoning out and inattentiveness came into play during my consult. I didn't even consider my lack of sleep being tied to ADHD. But now that I have a diagnosis, it explains quite a bit from my past. I wasn't just lazy and disorganized. Again, please go get tested if you suspect anything.\n",
    "\n",
    "Provide the answers in a structured format, directly listing the extracted features.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompte='''what is the capital of France? aswer the question in a single word\n",
    "   \n",
    "'''\n",
    "def get_mistral_response(prompt):\n",
    "    \n",
    "        # Replace with your Hugging Face API token\n",
    "    HUGGING_FACE_API_TOKEN = os.getenv(\"Mistrale_Token\")\n",
    "\n",
    "    # The API endpoint for the Mistral model\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "    #Limit to 1000 requests per day hence 1000 document per day \n",
    "\n",
    "\n",
    "    # Headers for authorization\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {HUGGING_FACE_API_TOKEN}\"\n",
    "    }\n",
    "\n",
    "    # Data to send to the model\n",
    "    data = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Send the POST request\n",
    "    response = requests.post(API_URL, headers=headers, json=data)\n",
    "\n",
    "    # Check the response\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_json_data(response):\n",
    "    response=response[0]['generated_text'].split('\\n\\n')[-1].split('\\n')\n",
    "    if response[0].startswith('--'):\n",
    "        print(response[0])\n",
    "        response=response[1:]\n",
    "    \n",
    "    # for el in response:\n",
    "    #     print(el)\n",
    "    #     print('\\n')\n",
    "    extracted_features = {\n",
    "    \"Sentiment\": response[0].split(':')[1].strip().split(' ')[0],\n",
    "    \"Topic\": response[1].split(':')[1].strip().split(' ')[0],\n",
    "    \"Personal_Experience\": response[2].split(':')[1].strip().split(' ')[0],\n",
    "    \"Mention of Solutions\": response[3].split(':')[1].strip().split(' ')[0],\n",
    "    \"Gender\": response[4].split(':')[1].strip().split(' ')[0],\n",
    "    \"Self-Diagnosis\": response[5].split(':')[1].strip().split(' ')[0],\n",
    "    \"Self-Medication\": response[6].split(':')[1].strip().split(' ')[0]\n",
    "    }\n",
    "\n",
    "    return extracted_features\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: This post is about a personal experience with a psychiatrist and medication, rather than technology, so I have marked the topic as null. The gender of the author is not explicitly stated in the post, but it can be inferred from a first name that is common for females, such as 'Amy', which is not mentioned. Therefore, I have marked the gender as female. I have also marked self-diagnosis and self-medication as null since there is no mention of them in the post.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m extracted_features\u001b[38;5;241m=\u001b[39maugmented_json_data(response)\n",
      "Cell \u001b[0;32mIn[94], line 12\u001b[0m, in \u001b[0;36maugmented_json_data\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(el)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m extracted_features \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPersonal_Experience\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMention of Solutions\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelf-Diagnosis\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelf-Medication\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;241m6\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     18\u001b[0m }\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extracted_features\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "extracted_features=augmented_json_data(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sentiment': 'Positive',\n",
       " 'Topic': 'Technology',\n",
       " 'Personal_Experience': 'Yes',\n",
       " 'Mention of Solutions': 'Null',\n",
       " 'Gender': 'Null',\n",
       " 'Self-Diagnosis': 'Yes',\n",
       " 'Self-Medication': 'Null'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(db, collection, limit=10):\n",
    "    # Get the documents from the specified collection\n",
    "    documents = db[collection].find({'sentiment': {'$exists': False}}, {'_id': 0}).limit(limit)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    UpdateOne({\"product_id\": 1}, {\"$set\": {\"discount\": 10}}),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(title, text):\n",
    "    prompt = f'''\n",
    "        Analyze the following Reddit post and provide concise answers to these features. \n",
    "        Use \"Yes\" or \"No\" for binary questions, and specify \"Null\" if information is unclear or not mentioned. Avoid explanations.\n",
    "\n",
    "        Features to extract:\n",
    "\n",
    "        Sentiment: positive/negative/neutral (if unclear put null)\n",
    "        Topic: Technology/medication/education/social \n",
    "        Personal Experience Shared: Yes/No\n",
    "        Mention of Solutions: Yes/No (if any solutions, advice, or recommendations are discussed just answer no explanation)\n",
    "        Gender of the Author: Male/Female/Null\n",
    "        Self-Diagnosis: Yes/No (keyword 'self-diagnosis')\n",
    "        Self-Medication: Yes/No (keyword 'self-medication' exits in this text ?)\n",
    "        Post for analysis:\n",
    "    \n",
    "        {text}\n",
    "\n",
    "\n",
    "\n",
    "        Provide the answers in a structured format like the following:\n",
    "        Sentiment: [answer]\n",
    "        Topic:  [answer]\n",
    "        Personal Experience Shared:  [answer]\n",
    "        Mention of Solutions:  [answer]\n",
    "        Gender of the Author:  [answer]\n",
    "        Self-Diagnosis:  [answer]\n",
    "        Self-Medication:  [answer]\n",
    "        \n",
    "        Do not leave \"[answer]\" as a response. Always replace it with one of the specified valid answers or \"Null.\"\n",
    "\n",
    "        '''\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to mongodb\n",
    "import pymongo\n",
    "def augment_documents():\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db=client['reddit_test']\n",
    "    documents=get_documents(db, 'reddit_posts', limit=10)\n",
    "    result_to_append=[]\n",
    "    for document in documents:\n",
    "        id=document['id']\n",
    "        print(f\"Processing document with id: {id}\")\n",
    "        print('\\n\\n')\n",
    "        title=document['title']\n",
    "        text=document['self_text']\n",
    "        try:\n",
    "            prompt=create_prompt(title, text)\n",
    "            response=get_mistral_response(prompt)\n",
    "            if(id=='1g65j41'):\n",
    "              print(response[0]['generated_text'])\n",
    "              return response\n",
    "            # print('ff')\n",
    "            response=augmented_json_data(response)\n",
    "            print(response)\n",
    "            \n",
    "            #db.reddit_posts.update_one({'id': id}, {'$set': response})\n",
    "            #print(f\"Updated document with id: {id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document with id: {id}\")\n",
    "            print(e)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document with id: 1et3kj0\n",
      "\n",
      "\n",
      "\n",
      "Error processing document with id: 1et3kj0\n",
      "list index out of range\n",
      "Processing document with id: 1hf8k5t\n",
      "\n",
      "\n",
      "\n",
      "{'Sentiment': 'Positive', 'Topic': 'Technology', 'Personal_Experience': 'Yes', 'Mention of Solutions': 'Null', 'Gender': 'Female', 'Self-Diagnosis': 'Null', 'Self-Medication': 'Null'}\n",
      "Processing document with id: 1f0k9en\n",
      "\n",
      "\n",
      "\n",
      "{'Sentiment': 'positive', 'Topic': 'Social', 'Personal_Experience': 'Yes', 'Mention of Solutions': 'Null', 'Gender': 'Null', 'Self-Diagnosis': 'Null', 'Self-Medication': 'Null'}\n",
      "Processing document with id: 1gk5ftv\n",
      "\n",
      "\n",
      "\n",
      "{'Sentiment': 'Neutral', 'Topic': 'Technology', 'Personal_Experience': 'No', 'Mention of Solutions': 'No', 'Gender': 'Null', 'Self-Diagnosis': 'Null', 'Self-Medication': 'Null'}\n",
      "Processing document with id: 1h5csgi\n",
      "\n",
      "\n",
      "\n",
      "{'Sentiment': 'Neutral', 'Topic': 'Technology', 'Personal_Experience': 'No', 'Mention of Solutions': 'No', 'Gender': 'Null', 'Self-Diagnosis': 'Null', 'Self-Medication': 'Null'}\n",
      "Processing document with id: 1h6aqhc\n",
      "\n",
      "\n",
      "\n",
      "{'Sentiment': 'Neutral', 'Topic': 'Technology', 'Personal_Experience': 'Yes', 'Mention of Solutions': 'No', 'Gender': 'Null', 'Self-Diagnosis': 'Yes', 'Self-Medication': 'Null'}\n",
      "Processing document with id: 1g7tbb2\n",
      "\n",
      "\n",
      "\n",
      "{'Sentiment': 'Neutral', 'Topic': 'Social', 'Personal_Experience': 'Yes', 'Mention of Solutions': 'Null', 'Gender': 'Null', 'Self-Diagnosis': 'Yes', 'Self-Medication': 'Null'}\n",
      "Processing document with id: 1dy8uqs\n",
      "\n",
      "\n",
      "\n",
      "{'Sentiment': 'Positive', 'Topic': 'Technology/null', 'Personal_Experience': 'Yes', 'Mention of Solutions': 'Yes', 'Gender': 'Female', 'Self-Diagnosis': 'Yes', 'Self-Medication': 'No'}\n",
      "Processing document with id: 1g65j41\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Analyze the following Reddit post and provide concise answers to these features. \n",
      "        Use \"Yes\" or \"No\" for binary questions, and specify \"Null\" if information is unclear or not mentioned. Avoid explanations.\n",
      "\n",
      "        Features to extract:\n",
      "\n",
      "        Sentiment: positive/negative/neutral (if unclear put null)\n",
      "        Topic: Technology/medication/education/social \n",
      "        Personal Experience Shared: Yes/No\n",
      "        Mention of Solutions: Yes/No (if any solutions, advice, or recommendations are discussed just answer no explanation)\n",
      "        Gender of the Author: Male/Female/Null\n",
      "        Self-Diagnosis: Yes/No (keyword 'self-diagnosis')\n",
      "        Self-Medication: Yes/No (keyword 'self-medication' exits in this text ?)\n",
      "        Post for analysis:\n",
      "    \n",
      "        I had my second visit with a new psychiatrist, and it went terribly. This provider, supposedly specializing in ADHD, called me a drug addict for advocating for myself.\n",
      "\n",
      "Stimulants were life-changing after 21 years of struggling. On Vyvanse, I became functional, confident, and better at relationships. I maintained the same dosage for over 2 years without tolerance issues.\n",
      "\n",
      "During the shortage, I went without for 6 weeks. Life became hell again. When I finally got pills, they were ineffective, even with increased dosages.\n",
      "\n",
      "I switched to Focalin, which worked even better than Vyvanse. I took it daily for 6 months without issues. Then, due to insurance changes, I had to see a new provider.\n",
      "\n",
      "This new provider insisted I \"strictly cannot\" take stimulants daily, despite my explanation of their benefits. She prescribed only 15 pills for over 30 days.\n",
      "\n",
      "Those 30 days were miserable. The Focalin lost effectiveness, and my productivity, self-esteem, and confidence plummeted.\n",
      "\n",
      "I brought this up in our second session, hoping she'd reconsider. Instead, she called me a \"drug addict because I am asking for more pills.\" When I mentioned my history of daily use without tolerance issues, she replied, \"I can only treat based on professional data.\" She ended by saying, \"I won't be prescribing controls of any type until I receive proof that you completed a substance abuse program.\"\n",
      "\n",
      "I'm not a drug addict! I have ADHD and need these meds to function. Why are doctors focused on treating the average person instead of the individual? Just because 85% of people build tolerance to stimulants doesn't mean the 15% who don't should be treated the same way.\n",
      "\n",
      "Needless to say, I'll be finding a new doctor.\n",
      "\n",
      "\n",
      "\n",
      "        Provide the answers in a structured format like the following:\n",
      "        Sentiment: [answer]\n",
      "        Topic:  [answer]\n",
      "        Personal Experience Shared:  [answer]\n",
      "        Mention of Solutions:  [answer]\n",
      "        Gender of the Author:  [answer]\n",
      "        Self-Diagnosis:  [answer]\n",
      "        Self-Medication:  [answer]\n",
      "        \n",
      "        Do not leave \"[answer]\" as a response. Always replace it with one of the specified valid answers or \"Null.\"\n",
      "\n",
      "         Sentiment: Negative\n",
      "         Topic: Technology/Null\n",
      "         Personal Experience Shared: Yes\n",
      "         Mention of Solutions: No\n",
      "         Gender of the Author: Female\n",
      "         Self-Diagnosis: Null\n",
      "         Self-Medication: No\n",
      "\n",
      "Note: This post is about a personal experience with a psychiatrist and medication, rather than technology, so I have marked the topic as null. The gender of the author is not explicitly stated in the post, but it can be inferred from a first name that is common for females, such as 'Amy', which is not mentioned. Therefore, I have marked the gender as female. I have also marked self-diagnosis and self-medication as null since there is no mention of them in the post.\n"
     ]
    }
   ],
   "source": [
    "response=augment_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after split Medication: Null\n",
      "['Medication: Null']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m augmented_json_data(response)\n",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m, in \u001b[0;36maugmented_json_data\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m      4\u001b[0m response\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m      6\u001b[0m extracted_features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPersonal_Experience\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMention of Solutions\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelf-Diagnosis\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;241m6\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelf-Medication\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;241m7\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extracted_features\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'Neutral', 'Topic': 'Social', 'Personal_Experience': 'Yes', 'Mention of Solutions': 'No', 'Gender': 'Null', 'Self-Diagnosis': 'Yes', 'Self-Medication': 'Null'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(augmented_json_data(test_response))\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Sentiment: Neutral\n",
      "        Topic: Social\n",
      "        Personal Experience Shared: Yes\n",
      "        Mention of Solutions: No\n",
      "        Gender of the Author: Null\n",
      "        Self-Diagnosis: Yes\n",
      "        Self-Medication: Null\n",
      "***************************\n",
      "Sentiment\n",
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "another_test_response=augmented_json_data(test_response)\n",
    "print(another_test_response)\n",
    "print('***************************')\n",
    "another_test_response=another_test_response.split('\\n')\n",
    "print(another_test_response[1].split(':')[0].strip().split(' ')[0])\n",
    "print(another_test_response[1].split(':')[1].strip().split(' ')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Sentiment: Neutral\n",
      "        Topic: Social\n",
      "        Personal Experience Shared: Yes\n",
      "        Mention of Solutions: No\n",
      "        Gender of the Author: Null\n",
      "        Self-Diagnosis: Yes\n",
      "        Self-Medication: Null\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(augmented_json_data(test_response))\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"does the keyword 'self-diagnosis' exits in this text ? in this \\n        Please go out and get tested if you are still on the fence. I always assumed ADHD was only hyperactive. A lot of concerns about day dreaming, zoning out and inattentiveness came into play during my consult. I didn't even consider my lack of sleep being tied to ADHD. But now that I have a diagnoses, it explains quite a bit from my past. I wasn't just lazy and disorganized. \\n\\n  \\nAgain, please go get tested if you suspect anything.\\n        give the answer in a single word\\n\\nNo\"}]\n"
     ]
    }
   ],
   "source": [
    "fine_tune=f'''does the keyword 'self-diagnosis' exits in this text ? in this \n",
    "        {text}\n",
    "        give the answer in a single word\n",
    "'''\n",
    "response=get_mistral_response(fine_tune)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sentiment: Positive',\n",
       " 'Topic: Technology (Null)',\n",
       " 'Personal Experience Shared: Yes',\n",
       " 'Mention of Solutions: Yes (Go out and get tested)',\n",
       " 'Gender of the Author: Female',\n",
       " 'Self-Diagnosis: Null',\n",
       " 'Self-Medication: Null']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]['generated_text'].strip().split('\\n\\n')[-1].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''As many of you are aware by now, the current U.S. shortage of medications used to treat ADHD has patients and parents of patients who rely on these medications scrambling to fill their prescriptions, leaving some people in a position where they are starting a new medicine or going without.\\n\\nDiscussion of the ongoing U.S. medication shortage is overwhelming the community and making it more difficult to discuss other topics; we have started this thread to contain all discussions until this shortage has ended. A moderator will remove any posts from here on out, and the moderation team will direct the user here. We will edit this post as vetted information becomes available.\\n\\n# [Joint Letter from FDA & DEA](https://www.fda.gov/media/170736/download)\\n\\n&#x200B;\\n\\n* If you are curious to see if there is a shortage of medication, the FDA provides access to their [shortage database](https://www.accessdata.fda.gov/scripts/drugshortages/dsp_SearchResults.cfm)\\n\\n**American Society of Health-System Pharmacists (ASHP) Shortage listings**\\n\\n**Adderall**\\n\\n* [Amphetamine mixed salts/Adderall XR](https://www.ashp.org/drug-shortages/current-shortages/drug-shortage-detail.aspx?id=863)\\n* [Amphetamine mixed salts/Adderall IR](https://www.ashp.org/drug-shortages/current-shortages/drug-shortage-detail.aspx?id=857)\\n\\n**Concerta**\\n\\n* [Methylphenidate ER/Concerta](https://www.ashp.org/drug-shortages/current-shortages/drug-shortage-detail.aspx?id=896)\\n* [Methylphenidate Immediate-Release Tablets](https://www.ashp.org/drug-shortages/current-shortages/drug-shortage-detail.aspx?id=973)\\n\\n**Focalin**\\n\\n* [Focalin XR](https://www.ashp.org/drug-shortages/current-shortages/drug-shortage-detail.aspx?id=955)\\n\\n**Intuniv**\\n\\n* [Guanfacine Hydrochloride Tablets](https://www.ashp.org/drug-shortages/current-shortages/drug-shortage-detail.aspx?id=508)\\n\\n**Vyvanse**\\n\\n* [Vyvanse Manufacturing Delay](https://www.vyvanse.com/supply)\\n* [Lisdexamfetamine Dimesylate Capsules](https://www.ashp.org/drug-shortages/current-shortages/drug-shortage-detail.aspx?id=961)\\n\\n&#x200B;\\n\\n**News Articles**\\n\\n* [Adderall shortage forces some patients to scramble, ration or go without](https://www.npr.org/sections/health-shots/2023/02/18/1157832613/adderall-shortage-forces-some-patients-to-scramble-ration-or-go-without) \\\\- NPR\\n* [Amid shortage of generic Adderall, frustration builds as demand increases](https://abcnews.go.com/Health/amid-shortage-generic-adderall-frustration-builds-demand-increases/story?id=98160584) \\\\- ABC\\n* [What Can Clinicians Do to Mitigate the ADHD Medication Shortage?](https://www.medpagetoday.com/video-coverage/adhd-video-insights/103833) \\\\- MedPage Today\\n* [Xanax and Adderall Access Is Being Blocked by Secret Drug Limits](https://www.bloomberg.com/news/articles/2023-04-03/adderall-shortages-are-made-worse-by-the-opioid-crisis?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTY4MDUzMzM5NCwiZXhwIjoxNjgxMTM4MTk0LCJhcnRpY2xlSWQiOiJSU0pLNlpEV0xVNjgwMSIsImJjb25uZWN0SWQiOiI2MjYxNzM0MDVGM0M0OUIwQTVERENENjVDOTYxNzZCRiJ9.-IR1D_I3w_JnKyX7Nxipci0VID1yFDxLd3WHKOTJVjM) \\\\- Bloomberg Gift Link\\n* [A â€˜perfect stormâ€™ led to an ADHD medication shortage. Hereâ€™s why](https://www.pbs.org/newshour/health/a-perfect-storm-led-to-an-adhd-medication-shortage-heres-why) \\\\- PBS News Hour\\n\\n**Community Posts**\\n\\n* [ðŸ’ŠHow to Outrun the Stimulant Medication ShortageðŸ’Š](https://www.reddit.com/r/ADHD/comments/11fm3v5/how_to_outrun_the_stimulant_medication_shortage/) \\\\- u/highway-dreamer\\n* [Should a class action lawsuit be filed against McKesson, AmerisourceBergen, and Cardinal for Causing the nationwide Adderall shortage with their monopoly on pharmaceutical distribution?](https://www.reddit.com/r/ADHD/comments/13lxi09/should_a_class_action_lawsuit_be_filed_against/) \\\\- u/peacockblueburgundy\\n\\n\\\\---\\n\\n**If you are having issues with the effectiveness of your meds and would like to report it, please see this** [post](https://www.reddit.com/r/ADHD/comments/112crp6/if_your_vyvanseadhd_meds_arent_working_like_they/)**.**\\n\\n* If you are in the UK, see [here](https://www.reddit.com/r/ADHD/comments/112crp6/comment/jb22dxf/).\\n\\nP.S.\\n\\nShire (insert other manufacturers) does not feed you poison inside Vyvanse capsules. Please stop the conspiracies, they are only stirring up more discontent in this difficult time.',\n",
    "    '''\n",
    "prompt=create_prompt('test', text)\n",
    "response=get_mistral_response(prompt)\n",
    "response_fixed=augmented_json_data(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'Neutral', 'Topic': 'Technology/Medication', 'Personal_Experience': 'No', 'Mention of Solutions': 'Yes', 'Gender': 'Null', 'Self-Diagnosis': 'Null', 'Self-Medication': 'Null'}\n"
     ]
    }
   ],
   "source": [
    "print(response_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_fixed=augmented_json_data(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'Positive', 'Topic': 'Technology', 'Personal_Experience': 'Yes', 'Mention of Solutions': 'Yes', 'Gender': 'Female', 'Self-Diagnosis': 'Null', 'Self-Medication': 'Null'}\n"
     ]
    }
   ],
   "source": [
    "print(res_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'Positive', 'Topic': 'Technology', 'Personal Experience Shared': 'Yes', 'Mention of Solutions': 'Yes', 'Gender of the Author': 'Female', 'Self-Diagnosis': 'No', 'Self-Medication': 'No'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "test=result.split('\\n')\n",
    "# Create a dictionary to hold the extracted features\n",
    "extracted_features = {\n",
    "    \"Sentiment\": test[0].split(':')[1].strip().split(' ')[0],\n",
    "    \"Topic\": test[1].split(':')[1].strip().split(' ')[0],\n",
    "    \"Personal_Experience\": test[2].split(':')[1].strip().split(' ')[0],\n",
    "    \"Mention of Solutions\": test[3].split(':')[1].strip().split(' ')[0],\n",
    "    \"Gender\": test[4].split(':')[1].strip().split(' ')[0],\n",
    "    \"Self-Diagnosis\": test[5].split(':')[1].strip().split(' ')[0],\n",
    "    \"Self-Medication\": test[6].split(':')[1].strip().split(' ')[0]\n",
    "}\n",
    "\n",
    "print(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Neutral\n",
      "Topic: Technology/Null\n",
      "Personal Experience Shared: Yes\n",
      "Mention of Solutions: Null\n",
      "Gender of the Author: Female\n",
      "Self-Diagnosis: Yes\n",
      "Self-Medication: Null\n"
     ]
    }
   ],
   "source": [
    "result=response.json()[0]['generated_text'].strip().split('\\n\\n')\n",
    "result=result[-1]\n",
    "# for el in result:\n",
    "#     print(el)\n",
    "    # print('\\n')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanning common LLM errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "def connect_to_mongo():\n",
    "    # Load environment variables from .env file\n",
    "    os.chdir('../../')      \n",
    "   \n",
    "      # MongoDB connection details\n",
    "    mongo_host = '127.0.0.1'  # Docker service name for MongoDB\n",
    "    mongo_port = 27017\n",
    "\n",
    "    try:\n",
    "        # Establish connection to MongoDB\n",
    "        client = MongoClient(host=mongo_host, port=mongo_port)\n",
    "        print(client.list_database_names())\n",
    "\n",
    "    \n",
    "        return client\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while connecting to MongoDB:\", e)\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admin', 'airflow_db', 'config', 'local', 'reddit', 'reddit_test']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "client=connect_to_mongo()\n",
    "db=client['reddit']\n",
    "get_errors_docs=db.reddit_posts.find({'Sentiment':''})\n",
    "nb=0\n",
    "for el in get_errors_docs:\n",
    "    print(el)\n",
    "    nb+=1\n",
    "print(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult({'n': 4, 'nModified': 4, 'ok': 1.0, 'updatedExisting': True}, acknowledged=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.reddit_posts.update_many({'Sentiment':''}, {'$unset': {'Topic': 1,'Sentiment': 1, 'Personal_Experience': 1, 'Mention of Solutions':1,'Gender':1,'Self-Diagnosis':1,'Self-Medication':1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents with sentiment field:  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find documents that have the 'Sentiment' field and those that don't\n",
    "documents_with_sentiment = db.reddit_posts.find({'$and': [{'Sentiment': {'$exists': False}}, {'Topic': {'$exists': True}}]})\n",
    "documents_without_sentiment = db.reddit_posts.find({'Sentiment': {'$exists': False}})\n",
    "nb=0\n",
    "for el in documents_with_sentiment:\n",
    "    print(el)\n",
    "    nb+=1\n",
    "print(\"number of documents with sentiment field: \", nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents without sentiment field:  8928\n"
     ]
    }
   ],
   "source": [
    "nb=0\n",
    "for el in documents_without_sentiment:\n",
    "    nb+=1\n",
    "print(\"number of documents without sentiment field: \", nb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
