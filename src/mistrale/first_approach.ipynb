{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first approach for Mistrale (Let's Hope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Do the login to hugging face and get the acess Tokin in order to get the model of mistral\n",
    "- !huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKIP THESE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'QuantizationConfig' from 'transformers' (/home/ysidhom/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel, QuantizationConfig\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Determine if CUDA (GPU) is available; if not, default to CPU\u001b[39;00m\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'QuantizationConfig' from 'transformers' (/home/ysidhom/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "\n",
    "# Determine if CUDA (GPU) is available; if not, default to CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for loading the model in 4-bit precision to save memory\n",
    "bitsquant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Load model in 4-bit precision\n",
    "    bnb_4bit_use_double_quant=True,       # Use double quantization for better compression\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Specify quantization type as NF4 (a specific format for quantization)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 as the compute data type for operations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf1ea2598b948a885132fb73b70d4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061f5332d9de4c42adc0f7e68826a96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a7c6ddb8c148e5bd070ae3cf518013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b26656a3c0436784f4a920f4a0184f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00008.bin:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ba3ca4a6d54ac192c048e1359dd067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00008.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1d9aeeda014a3f935d48649bea3404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443fe57c0b0e44e98a924433a4ed6bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00008.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c19e3cd83c4c8985848c82e08b62e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f4e0a8287c44018042dae190ec6a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00008.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e598c6398904d978410b6736dc3399a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00007-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7b00ff3c9e4458ad61b45806d17c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00008-of-00008.bin:   0%|          | 0.00/816M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4766566c7d49589089bbb60b02359c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7835efb6f48049b8b6de1922ddbdb424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Configuration for loading the model in 4-bit precision to save memory\n",
    "bitsquant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Load model in 4-bit precision\n",
    "    bnb_4bit_use_double_quant=True,       # Use double quantization for better compression\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Specify quantization type as NF4 (a specific format for quantization)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 as the compute data type for operations\n",
    ")\n",
    "\n",
    "# mistral ai repository for Mistral-7B with the original tuned weights\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "sharded_id = \"filipealmeida/Mistral-7B-v0.1-sharded\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    sharded_id,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bitsquant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./saved_model\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer to a directory\n",
    "save_path = \"./saved_model\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_6134/3201874984.py:17: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  mistral_llm = HuggingFacePipeline(pipeline=text_gen_pipeline)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "text_gen_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.85, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    pad_token_id=tokenizer.eos_token_id, \n",
    "    repetition_penalty=1.1, \n",
    "    return_full_text=True,\n",
    "    max_new_tokens=256,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_gen_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ysidhom/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.85` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is the usual composition of rocket fuel?\\n\\nThe most common propellants are solid, liquid and hybrid. Solid propellant is a mixture of oxidizer and fuel that is cast into a solid shape. Liquid propellant is a combination of oxidizer and fuel that is stored in separate tanks and mixed just before use. Hybrid propellant is a mixture of oxidizer and fuel that is stored separately but ignited together.\\n\\nWhat is the difference between a rocket and a missile?\\n\\nA rocket is an unguided projectile that flies through space. A missile is a guided projectile that flies through space.\\n\\nHow do you make a homemade rocket?\\n\\nYou can make a homemade rocket by using a soda bottle, a balloon, some tape, and a few other materials. First, cut off the top of the soda bottle and remove the label. Then, blow up the balloon and tie it to the neck of the bottle. Next, tape the balloon to the bottom of the bottle. Finally, add water to the bottle until it is about half full. When you are ready to launch your rocket, simply release the balloon and watch it fly!\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What is the usual composition of rocket fuel?\"\n",
    "mistral_llm.invoke(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyze the following Reddit post and provide concise answers to these features. \n",
      "Use \"Yes\" or \"No\" for binary questions, and specify \"Null\" if information is unclear or not mentioned. Avoid lengthy explanations.\n",
      "\n",
      "Features to extract:\n",
      "\n",
      "Sentiment: Is the sentiment positive, negative, or neutral?\n",
      "Topic: Identify the main topic(s) of the post (be specific and avoid general terms like \"ADHD\").\n",
      "Personal Experience Shared: Does the post mention a personal experience?\n",
      "Mention of Solutions: Are any solutions, advice, or recommendations discussed?\n",
      "Self-Diagnosis: Is there any indication of self-diagnosis?\n",
      "Gender of the Author: Can the gender of the author be inferred?\n",
      "Self-Medication: Is there any mention of self-medication?\n",
      "Post for analysis:\n",
      "Diagnosed with Inattentive ADHD at 31. Explains so many things from my childhood. Please go out and get tested if you are still on the fence. I always assumed ADHD was only hyperactive. A lot of concerns about day dreaming, zoning out and inattentiveness came into play during my consult. I didn't even consider my lack of sleep being tied to ADHD. But now that I have a diagnosis, it explains quite a bit from my past. I wasn't just lazy and disorganized. Again, please go get tested if you suspect anything.\n",
      "\n",
      "Provide the answers in a structured format, directly listing the extracted features..\n"
     ]
    }
   ],
   "source": [
    "text='''\n",
    "Analyze the following Reddit post and provide concise answers to these features. \n",
    "Use \"Yes\" or \"No\" for binary questions, and specify \"Null\" if information is unclear or not mentioned. Avoid lengthy explanations.\n",
    "\n",
    "Features to extract:\n",
    "\n",
    "Sentiment: Is the sentiment positive, negative, or neutral?\n",
    "Topic: Identify the main topic(s) of the post (be specific and avoid general terms like \"ADHD\").\n",
    "Personal Experience Shared: Does the post mention a personal experience?\n",
    "Mention of Solutions: Are any solutions, advice, or recommendations discussed?\n",
    "Self-Diagnosis: Is there any indication of self-diagnosis?\n",
    "Gender of the Author: Can the gender of the author be inferred?\n",
    "Self-Medication: Is there any mention of self-medication?\n",
    "Post for analysis:\n",
    "Diagnosed with Inattentive ADHD at 31. Explains so many things from my childhood. Please go out and get tested if you are still on the fence. I always assumed ADHD was only hyperactive. A lot of concerns about day dreaming, zoning out and inattentiveness came into play during my consult. I didn't even consider my lack of sleep being tied to ADHD. But now that I have a diagnosis, it explains quite a bit from my past. I wasn't just lazy and disorganized. Again, please go get tested if you suspect anything.\n",
    "\n",
    "Provide the answers in a structured format, directly listing the extracted features..'''\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ysidhom/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.85` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "outputs=mistral_llm.invoke(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyze the following Reddit post and provide concise answers to these features. \n",
      "Use \"Yes\" or \"No\" for binary questions, and specify \"Null\" if information is unclear or not mentioned. Avoid lengthy explanations.\n",
      "\n",
      "Features to extract:\n",
      "\n",
      "Sentiment: Is the sentiment positive, negative, or neutral?\n",
      "Topic: Identify the main topic(s) of the post (be specific and avoid general terms like \"ADHD\").\n",
      "Personal Experience Shared: Does the post mention a personal experience?\n",
      "Mention of Solutions: Are any solutions, advice, or recommendations discussed?\n",
      "Self-Diagnosis: Is there any indication of self-diagnosis?\n",
      "Gender of the Author: Can the gender of the author be inferred?\n",
      "Self-Medication: Is there any mention of self-medication?\n",
      "Post for analysis:\n",
      "Diagnosed with Inattentive ADHD at 31. Explains so many things from my childhood. Please go out and get tested if you are still on the fence. I always assumed ADHD was only hyperactive. A lot of concerns about day dreaming, zoning out and inattentiveness came into play during my consult. I didn't even consider my lack of sleep being tied to ADHD. But now that I have a diagnosis, it explains quite a bit from my past. I wasn't just lazy and disorganized. Again, please go get tested if you suspect anything.\n",
      "\n",
      "Provide the answers in a structured format, directly listing the extracted features..\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def get_llm(text_prompt: str) -> str:\n",
    "    return mistral_llm.invoke(text_prompt)\n",
    "\n",
    "# Define the column width for text wrapping \n",
    "# (Medium's typical width is around 80 characters)\n",
    "COLUMN_WIDTH: int = 76\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Remove newline characters and any other special characters\n",
    "    cleaned_text = text.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def wrap_text(text: str, width: int) -> str:\n",
    "    # Use textwrap to wrap text to the specified width\n",
    "    wrapped_text = textwrap.fill(text, width=width)\n",
    "    return wrapped_text\n",
    "\n",
    "def wrap_text_with_comments(text: str, width: int) -> str:\n",
    "    # First wrap the text to specified width, which is < 80 char\n",
    "    # by which we create out text lines\n",
    "    wrapped_text = textwrap.fill(text, width=width)\n",
    "\n",
    "    # Split the wrapped text\n",
    "    lines = wrapped_text.split('\\n')\n",
    "\n",
    "    # Add # character for each line and join the lines\n",
    "    commented_lines = ['# ' + line for line in lines]\n",
    "    commented_text = '\\n'.join(commented_lines)\n",
    "    return commented_text\n",
    "\n",
    "def get_llm_response(text_prompt: str) -> str:\n",
    "    # Get the response from the model\n",
    "    #response = mistral_llm.invoke(text_prompt)\n",
    "    # Clean the output text\n",
    "    cleaned_response = clean_text(text_prompt)\n",
    "    # Wrap the cleaned response text to the specified column width\n",
    "    wrapped_response = wrap_text_with_comments(cleaned_response, COLUMN_WIDTH)\n",
    "    return wrapped_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Analyze the following Reddit post and extract the following features: -\n",
      "# Sentiment: Is the sentiment of the post positive, negative, or neutral? -\n",
      "# Topic: What are the main topic discussed in the post (do not put adhd I want\n",
      "# someting more specific? - Personal experience shared? (Yes/No): - Mention of\n",
      "# solutions? (Yes/No): -self diagnosis? -gendre of person? -self mediaction?\n",
      "# â†’ If there is no clear mention put null and answer only in yes no to\n",
      "# explanation required Post: Diagnosed with Inattentive ADHD at 31. Explains\n",
      "# so many things from my childhood. Please go out and get tested if you are\n",
      "# still on the fence. I always assumed ADHD was only hyperactive. A lot of\n",
      "# concerns about day dreaming, zoning out and inattentiveness came into play\n",
      "# during my consult. I didn't even consider my lack of sleep being tied to\n",
      "# ADHD. But now that I have a diagnoses, it explains quite a bit from my past.\n",
      "# I wasn't just lazy and disorganized. Again, please go get tested if you\n",
      "# suspect anything. It can be life changing.\n"
     ]
    }
   ],
   "source": [
    "clean_response = get_llm_response(outputs)\n",
    "print(clean_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_llm_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clean_response \u001b[38;5;241m=\u001b[39m get_llm_response(text)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(clean_response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_llm_response' is not defined"
     ]
    }
   ],
   "source": [
    "clean_response = get_llm_response(text)\n",
    "print(clean_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start To Focus Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Replace with your Hugging Face API token\n",
    "HUGGING_FACE_API_TOKEN = \"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\"\n",
    "\n",
    "# The API endpoint for the Mistral model\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "#Limit to 1000 requests per day hence 1000 document per day \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your input prompt\n",
    "prompt = '''\n",
    "Analyze the following Reddit post and provide concise answers to these features. \n",
    "Use \"Yes\" or \"No\" for binary questions, and specify \"Null\" if information is unclear or not mentioned. Avoid explanations.\n",
    "\n",
    "Features to extract:\n",
    "\n",
    "Sentiment: positive/negative/neutral\n",
    "Topic: Technology/medication/education/social \n",
    "Personal Experience Shared: Yes/No\n",
    "Mention of Solutions: Yes/No (if any solutions, advice, or recommendations are discussed just answer no explanation)\n",
    "Gender of the Author: Male/Female/Null\n",
    "Self-Diagnosis: Yes/No (if there is any indication of self-diagnosis)\n",
    "Self-Medication: Yes/No\n",
    "Post for analysis:\n",
    "32F Diagnosed with Inattentive ADHD at 31. Explains so many things from my childhood. Please go out and get tested if you are still on the fence. I always assumed ADHD was only hyperactive. A lot of concerns about day dreaming, zoning out and inattentiveness came into play during my consult. I didn't even consider my lack of sleep being tied to ADHD. But now that I have a diagnosis, it explains quite a bit from my past. I wasn't just lazy and disorganized. Again, please go get tested if you suspect anything.\n",
    "\n",
    "Provide the answers in a structured format, directly listing the extracted features.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response:\n"
     ]
    }
   ],
   "source": [
    "prompte='''what is the capital of France? aswer the question in a single word\n",
    "   \n",
    "'''\n",
    "# Headers for authorization\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {HUGGING_FACE_API_TOKEN}\"\n",
    "}\n",
    "\n",
    "# Data to send to the model\n",
    "data = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the POST request\n",
    "response = requests.post(API_URL, headers=headers, json=data)\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    print(\"Model Response:\")\n",
    "    result=response.json()\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\n",
      "Topic: Technology (implicit, not explicitly stated)\n",
      "Personal Experience Shared: Yes\n",
      "Mention of Solutions: Yes (diagnosis and getting tested)\n",
      "Gender of the Author: Female\n",
      "Self-Diagnosis: No\n",
      "Self-Medication: No (not mentioned)\n"
     ]
    }
   ],
   "source": [
    "result=response.json()[0]['generated_text'].strip().split('\\n\\n')\n",
    "result=result[-1]\n",
    "# for el in result:\n",
    "#     print(el)\n",
    "    # print('\\n')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'Positive', 'Topic': 'Technology', 'Personal Experience Shared': 'Yes', 'Mention of Solutions': 'Yes', 'Gender of the Author': 'Female', 'Self-Diagnosis': 'No', 'Self-Medication': 'No'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "test=result.split('\\n')\n",
    "# Create a dictionary to hold the extracted features\n",
    "extracted_features = {\n",
    "    \"Sentiment\": test[0].split(':')[1].strip().split(' ')[0],\n",
    "    \"Topic\": test[1].split(':')[1].strip().split(' ')[0],\n",
    "    \"Personal_Experience\": test[2].split(':')[1].strip().split(' ')[0],\n",
    "    \"Mention of Solutions\": test[3].split(':')[1].strip().split(' ')[0],\n",
    "    \"Gender\": test[4].split(':')[1].strip().split(' ')[0],\n",
    "    \"Self-Diagnosis\": test[5].split(':')[1].strip().split(' ')[0],\n",
    "    \"Self-Medication\": test[6].split(':')[1].strip().split(' ')[0]\n",
    "}\n",
    "\n",
    "print(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Neutral\n",
      "Topic: Technology/Null\n",
      "Personal Experience Shared: Yes\n",
      "Mention of Solutions: Null\n",
      "Gender of the Author: Female\n",
      "Self-Diagnosis: Yes\n",
      "Self-Medication: Null\n"
     ]
    }
   ],
   "source": [
    "result=response.json()[0]['generated_text'].strip().split('\\n\\n')\n",
    "result=result[-1]\n",
    "# for el in result:\n",
    "#     print(el)\n",
    "    # print('\\n')\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
