{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit API\n",
    "- In the following document we will try to extract reddit post:\n",
    "    - As a first approach we will collect data marked by keyboards relatedt to adhd found in related subreddits\n",
    "    - To improve our model we can modify our model to identify by it's own (using old data for example) the subreddits that could be intereesting \n",
    "    to scrap.\n",
    "\n",
    "\n",
    "    General composition of a post:\n",
    "    -  id: The post’s ID\n",
    "    -  title: The post’s title\n",
    "    -  text: The post’s text\n",
    "    -  author: The post’s author\n",
    "    -  created_utc: The post’s creation time in UTC\n",
    "    -  score: The post’s score\n",
    "    -  num_comments: The number of comments on the post\n",
    "    -  permalink: The post’s permalink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to the Reddit APi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected! Logged in as: ProfessorMiddle1326\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import praw\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "        client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "        user_agent=os.getenv(\"REDDIT_USER_AGENT\"),\n",
    "        username=os.getenv(\"REDDIT_USERNAME\"),\n",
    "        password=os.getenv(\"REDDIT_PASSWORD\")\n",
    "    )\n",
    "    print(f\"Connected! Logged in as: {reddit.user.me()}\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleanning and Tokenization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's first try to check for common misspelling of the words in the text and then proceed to clean it\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "def correct_spelling(text):\n",
    "    \"\"\"\n",
    "    Correct common misspellings in a text using pyspellchecker.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to correct.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with corrected spelling.\n",
    "        \n",
    "    \"\"\"\n",
    "    spell = SpellChecker()\n",
    "    words = text.split()\n",
    "    corrected_words = [spell.correction(word) if word in spell else word for word in words]\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "        #First let's define the cleanning function\n",
    "    import re\n",
    "    import string\n",
    "    import nltk\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    \n",
    "    text=correct_spelling(text)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    #text = re.sub(r'[.]', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    text = \" \".join([stemmer.stem(word) for word in text.split()])     \n",
    "    return text\n",
    "\n",
    "def contains_keywords(text, keywords):\n",
    "    \"\"\"\n",
    "    Check if a given text contains at least one keyword from a list after correcting spelling.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to check.\n",
    "        keywords (list): A list of keywords to look for.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if at least one keyword is found, False otherwise.\n",
    "    \"\"\"\n",
    "    # Correct spelling in the text\n",
    "    corrected_text = correct_spelling(text)\n",
    "    \n",
    "    # Convert corrected text to lowercase for case-insensitive matching\n",
    "    corrected_text = corrected_text.lower()\n",
    "    \n",
    "    # Check if any keyword is in the corrected text\n",
    "    for word in keywords:\n",
    "        if word.lower() in corrected_text:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Scrap to gather keywords \n",
    "- In this appraoch we will try to tokenize the 100 top posts in the subreddit ADHD\n",
    "- Once we get these keyborads this would help us filter new post into either posts about ADHD or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title  \\\n",
      "0                     How I cured my adhd permanently   \n",
      "1   I went through 700 reddit comments and collect...   \n",
      "2   ADHD for me is laying down on my couch using m...   \n",
      "3   It feels like there aren’t enough hours in the...   \n",
      "4   It's so damn irritating to be intelligent with...   \n",
      "..                                                ...   \n",
      "95  Universities move online amid COVID19, create ...   \n",
      "96  I have fake conversations in my head all day long   \n",
      "97  I'm gonna do it. 2023 is the year I start coll...   \n",
      "98  Every evening I feel like I wasted my entire d...   \n",
      "99  I’ve brushed my teeth for twenty-seven days st...   \n",
      "\n",
      "                                                 text  \n",
      "0   I've been suffering from adhd my whole life, f...  \n",
      "1   So there was that awesome [Reddit thread](http...  \n",
      "2   It’s not like I don’t care. I’m stressed out o...  \n",
      "3                                         IM OVER IT.  \n",
      "4   So I've always been told I'm smart by people w...  \n",
      "..                                                ...  \n",
      "95  My university cancelled all in-person classes ...  \n",
      "96  My mind is a constant whirlwind of thoughts. L...  \n",
      "97  My community college offers an associates in m...  \n",
      "98  Every evening I feel like I wasted my entire d...  \n",
      "99  I know that sounds bad, like did you not brush...  \n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Subreddit to target\n",
    "subreddit_name = \"ADHD\"\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "# Fetch posts\n",
    "posts = []\n",
    "for post in subreddit.top(limit=100):# 'hot', 'new', or 'top' post    \n",
    "    posts.append({\n",
    "        \"title\": post.title,\n",
    "        \"text\": post.selftext,\n",
    "    })\n",
    "    \n",
    "# extract from the posts the keywords\n",
    "df=pd.DataFrame(posts)\n",
    "df['text_cleaned'] = df['text'].apply(clean_text)\n",
    "df['title_cleaned'] = df['title'].apply(clean_text)\n",
    "df_tokenize = pd.DataFrame(index=df.index)\n",
    "df_tokenize['text'] = df['text_cleaned']+df['title_cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 interesting keywords based on TF-IDF scores:\n",
      "\n",
      "\n",
      "| adhd  | like  | feel  | time  | peopl  | know  | fuck  | work  | day  | life  \n",
      "\n",
      "| thing  | someth  | need  | start  | make  | help  | hour  | edit  | use  | want  \n",
      "\n",
      "| tri  | everi  | think  | realli  | task  | way  | actual  | someon  | thank  | read  \n",
      "\n",
      "| person  | post  | anyth  | minut  | brain  | say  | got  | said  | year  | told  \n",
      "\n",
      "| mani  | good  | word  | right  | mean  | disord  | went  | save  | check  | everyth  \n",
      "\n",
      "| struggl  | attent  | alway  | watch  | thought  | guy  | pm  | focu  | kid  | better  \n",
      "\n",
      "| ask  | execut  | els  | night  | end  | bad  | parent  | stuff  | lot  | love  \n",
      "\n",
      "| eat  | self  | instead  | stop  | anyon  | dysfunct  | bed  | spend  | phone  | relat  \n",
      "\n",
      "| write  | child  | care  | learn  | pay  | sleep  | tell  | remind  | talk  | late  \n",
      "\n",
      "| shit  | forgot  | dish  | noth  | today  | week  | lazi  | abil  | hard  | point  "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the cleaned text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_tokenize['text'])\n",
    "\n",
    "# Get the feature names (tokens)\n",
    "tokens = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sum up the TF-IDF scores of each token\n",
    "tfidf_scores = np.asarray(tfidf_matrix.sum(axis=0)).flatten()\n",
    "\n",
    "# Create a DataFrame with tokens and their TF-IDF scores\n",
    "tfidf_df = pd.DataFrame({'token': tokens, 'score': tfidf_scores})\n",
    "\n",
    "# Sort the DataFrame by TF-IDF score in descending order and get the top 10 tokens\n",
    "top_100_tfidf = tfidf_df.sort_values(by='score', ascending=False).head(100)\n",
    "\n",
    "keywords = top_50_tfidf['token'].tolist()\n",
    "print(\"Top 100 interesting keywords based on TF-IDF scores:\")   \n",
    "for i in range(100):\n",
    "    if i%10==0:\n",
    "        print('\\n')\n",
    "    print(f\"| {keywords[i]} \", end=\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Now let's manually select the common words that better adress our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Keywords: ['adhd', 'diagnose', 'energy', 'brain', 'test', 'distracted', 'forgetful', 'doctor', 'work', 'task', 'disord', 'struggl', 'focu', 'dysfunct', 'forgot', 'lazi', 'prescrib', 'medic', 'medicin', 'pill']\n",
      "Final Stemmed Keywords: ['adhd', 'diagnos', 'energi', 'brain', 'test', 'distract', 'forget', 'doctor', 'work', 'task', 'disord', 'struggl', 'focu', 'dysfunct', 'forgot', 'lazi', 'prescrib', 'medic', 'medicin', 'pill']\n"
     ]
    }
   ],
   "source": [
    "final_keywords = [\"adhd\", \"diagnose\",\"energy\", \"brain\", \"test\", \"distracted\", \"forgetful\", \"doctor\"\n",
    "                  ,\"work\",\"task\",\"disord\",\"struggl\",\"focu\",\"dysfunct\",\"forgot\",\"lazi\",\"prescrib\",\"medic\",\"medicin\",\"pill\"]\n",
    "\n",
    "final_stemmed_keywords = [ps.stem(word) for word in final_keywords]\n",
    "print(\"Final Keywords:\", final_keywords)\n",
    "print(\"Final Stemmed Keywords:\", final_stemmed_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's use these words as a first filter to posts (talking about ADHD or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
