{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAMA model \n",
    "- Classify the seriousness of the subredits\n",
    "- Classify the reddit post  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (2.5.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torchvision in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (0.26.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from huggingface_hub) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fasfous\\anaconda3\\envs\\llama_env\\lib\\site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n",
    "%pip install transformers\n",
    "# Make sure you have Hugging Face account and token for accessing LLaMA models\n",
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the model locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\Fasfous\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "login(token=os.getenv(\"HugginFace_Token\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Specify the model repository\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Download the tokenizer and model with progress tracking\n",
    "print(\"Downloading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, progress_bar=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    local_files_only=False,  # This means it will download the model from HuggingFace Hub\n",
    "    token=True,  # If necessary for private models\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model locally...\n",
      "Saving tokenizer locally...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./Llama-3.2-1B\\\\tokenizer_config.json',\n",
       " './Llama-3.2-1B\\\\special_tokens_map.json',\n",
       " './Llama-3.2-1B\\\\tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save locally if needed\n",
    "print(\"Saving model locally...\")\n",
    "model.save_pretrained(\"./Llama-3.2-1B\")\n",
    "\n",
    "print(\"Saving tokenizer locally...\")\n",
    "tokenizer.save_pretrained(\"./Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tunning the Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fasfous\\anaconda3\\envs\\llama_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Reddit dataset (example: pushshift dataset)\n",
    "dataset = load_dataset(\"jsfactory/mental_health_reddit_posts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset[\"train\"]))\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Take only 10% of the dataset\n",
    "dataset = dataset['train'].shuffle(seed=42).select(range(int(0.1 * len(dataset[\"train\"]))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset# Split into train and eval (80% for training, 20% for evaluation)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920\n",
      "Dataset({\n",
      "    features: ['body', 'subreddit'],\n",
      "    num_rows: 480\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1920/1920 [00:00<00:00, 6075.32 examples/s]\n",
      "\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 480/480 [00:00<00:00, 5922.89 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1920/1920 [00:00<00:00, 3522.90 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 480/480 [00:00<00:00, 3220.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"body\"]\n",
    "    inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def preprocess_data(example):\n",
    "    return tokenizer(\n",
    "        example[\"body\"],  # Replace \"body\" with the field containing the Reddit post\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,  # Adjust based on LLaMA's context length\n",
    "    )\n",
    "\n",
    "model_path = \"./Llama-3.2-1B\"\n",
    "# Map the preprocessing function to the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token if not defined\n",
    "\n",
    "tokenized_test_datasets = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_datasets = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
    "eval_dataset = eval_dataset.map(preprocess_data, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128256, 2048)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load your LLaMA model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model.resize_token_embeddings(len(tokenizer))  # Adjust token embeddings if vocabulary changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_llama\",  # Output directory for model checkpoints\n",
    "    eval_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",  # Save model after each epoch\n",
    "    save_total_limit=2,  # Limit to two model checkpoints\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Trainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# Create a DataCollator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # Pass the eval dataset here\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch CUDA available: True\n",
      "Torch CUDA version: 12.1\n",
      "Device name: NVIDIA GeForce GTX 1050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "print(\"Torch CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Torch CUDA version:\", torch.version.cuda)\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA device found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import torch\n",
    "\n",
    "# Ensure CUDA is available\n",
    "if not torch.cuda.is_available():\n",
    "    raise EnvironmentError(\"GPU is not available. Ensure CUDA is installed and PyTorch is using it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model and tokenizer\n",
    "model_path = \"./Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\")  # Move model to GPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a DataCollator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fasfous\\anaconda3\\envs\\llama_env\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define Training Arguments with GPU support\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_llama\",  # Output directory for model checkpoints\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,   # Batch size per GPU\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",           # Save model after each epoch\n",
    "    save_total_limit=2,              # Limit to two model checkpoints\n",
    "    logging_dir=\"./logs\",            # Directory for logs\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",                # Disable reporting to external services like WandB\n",
    "    fp16=True,                       # Enable mixed precision for faster training on GPUs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 9.23 GiB is allocated by PyTorch, and 929.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 18\u001b[0m\n\u001b[0;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      7\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      8\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     processing_class\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Fasfous\\anaconda3\\envs\\llama_env\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2165\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2166\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2167\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2168\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2169\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Fasfous\\anaconda3\\envs\\llama_env\\Lib\\site-packages\\transformers\\trainer.py:2522\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2516\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2517\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2519\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2520\u001b[0m )\n\u001b[0;32m   2521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2522\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2525\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2526\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2528\u001b[0m ):\n\u001b[0;32m   2529\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2530\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Fasfous\\anaconda3\\envs\\llama_env\\Lib\\site-packages\\transformers\\trainer.py:3688\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3686\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3687\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3689\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Fasfous\\anaconda3\\envs\\llama_env\\Lib\\site-packages\\accelerate\\accelerator.py:2244\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\Fasfous\\anaconda3\\envs\\llama_env\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Fasfous\\anaconda3\\envs\\llama_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Fasfous\\anaconda3\\envs\\llama_env\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 9.23 GiB is allocated by PyTorch, and 929.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Ensure the tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_test_datasets,\n",
    "    eval_dataset=tokenized_eval_datasets,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another test for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA memory cleaned.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reset memory\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_max_memory_cached()\n",
    "\n",
    "print(\"CUDA memory cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model and tokenizer\n",
    "model_path = \"./Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)  # Move model to GPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fasfous\\anaconda3\\envs\\adhdconda\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a DataCollator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_llama\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Enable mixed precision for faster GPU training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure the tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Set the environment variable to avoid OpenMP runtime error\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Set the environment variable to avoid fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_path = \"./Llama-3.2-1B\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)  # Move model to GPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Ensure the tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Reduce the batch size in your training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,  # Adjust this value based on your GPU memory\n",
    "    per_device_eval_batch_size=4,   # Adjust this value based on your GPU memory\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1050 (Device ID: 0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device ID\n",
    "    current_device = torch.cuda.current_device()\n",
    "    # Get the name of the GPU\n",
    "    gpu_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Using GPU: {gpu_name} (Device ID: {current_device})\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question with a single word: yes or no.\n",
      "Question: Is Paris the Capital of Spain?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model (already saved locally)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Llama-3.2-1B\")\n",
    "\n",
    "# Set pad token ID to eos token ID if pad token ID is not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Test the model\n",
    "input_text = (\n",
    "    \"Answer the following question with a single word: yes or no.\\n\"\n",
    "    \"Question: Is Paris the Capital of Spain?\"\n",
    ")\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = input_ids.ne(tokenizer.pad_token_id).long()\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(input_ids, attention_mask=attention_mask, max_length=50, pad_token_id=tokenizer.eos_token_id)\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Mongdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MongoDB successfully!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "mongo='127.0.0.1'\n",
    "\n",
    "try:\n",
    "    # Connect to MongoDB\n",
    "    myclient = MongoClient(\n",
    "                        \"mongodb://\"+mongo+\":27017/\",  \n",
    "                        username='admin',\n",
    "                        password='admin') #Mongo URI format\n",
    "    db=myclient['reddit']\n",
    "    print(\"Connected to MongoDB successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while connecting to MongoDB:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_at</th>\n",
       "      <th>self_text</th>\n",
       "      <th>searchQuery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1et3kj0</td>\n",
       "      <td>Diagnosed with Inattentive ADHD at 31. Explain...</td>\n",
       "      <td>amadnomad</td>\n",
       "      <td>392</td>\n",
       "      <td>107</td>\n",
       "      <td>0.98</td>\n",
       "      <td>https://www.reddit.com/r/ADHD/comments/1et3kj0...</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>1.723749e+09</td>\n",
       "      <td>Please go out and get tested if you are still ...</td>\n",
       "      <td>adhd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1gk5ftv</td>\n",
       "      <td>Children with higher IQ scores were diagnosed ...</td>\n",
       "      <td>Pretend_Voice_3140</td>\n",
       "      <td>4499</td>\n",
       "      <td>466</td>\n",
       "      <td>0.99</td>\n",
       "      <td>https://www.reddit.com/r/ADHD/comments/1gk5ftv...</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>1.730809e+09</td>\n",
       "      <td>A study was published in the [British Journal ...</td>\n",
       "      <td>adhd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1f0k9en</td>\n",
       "      <td>Reminder: If you made it to adulthood with lat...</td>\n",
       "      <td>Hipster-Deuxbag</td>\n",
       "      <td>7372</td>\n",
       "      <td>535</td>\n",
       "      <td>0.99</td>\n",
       "      <td>https://www.reddit.com/r/ADHD/comments/1f0k9en...</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>1.724547e+09</td>\n",
       "      <td>We all know the statistics: 20,000 behavioral ...</td>\n",
       "      <td>adhd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1g7tbb2</td>\n",
       "      <td>What is the most adhd thing you have ever adhdâ€™ed</td>\n",
       "      <td>FamiliarRadio9275</td>\n",
       "      <td>2102</td>\n",
       "      <td>1022</td>\n",
       "      <td>0.99</td>\n",
       "      <td>https://www.reddit.com/r/ADHD/comments/1g7tbb2...</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>1.729409e+09</td>\n",
       "      <td>I laugh so hard when I look back at this memor...</td>\n",
       "      <td>adhd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1dy8uqs</td>\n",
       "      <td>Iâ€™m angry that no one recognized that I had AD...</td>\n",
       "      <td>thecynicalone26</td>\n",
       "      <td>3345</td>\n",
       "      <td>598</td>\n",
       "      <td>0.99</td>\n",
       "      <td>https://www.reddit.com/r/ADHD/comments/1dy8uqs...</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>1.720445e+09</td>\n",
       "      <td>I just got diagnosed, and Iâ€™m 39.  My entire l...</td>\n",
       "      <td>adhd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  1et3kj0  Diagnosed with Inattentive ADHD at 31. Explain...   \n",
       "1  1gk5ftv  Children with higher IQ scores were diagnosed ...   \n",
       "2  1f0k9en  Reminder: If you made it to adulthood with lat...   \n",
       "3  1g7tbb2  What is the most adhd thing you have ever adhdâ€™ed   \n",
       "4  1dy8uqs  Iâ€™m angry that no one recognized that I had AD...   \n",
       "\n",
       "               author  score  num_comments  upvote_ratio  \\\n",
       "0           amadnomad    392           107          0.98   \n",
       "1  Pretend_Voice_3140   4499           466          0.99   \n",
       "2     Hipster-Deuxbag   7372           535          0.99   \n",
       "3   FamiliarRadio9275   2102          1022          0.99   \n",
       "4     thecynicalone26   3345           598          0.99   \n",
       "\n",
       "                                                 url subreddit    created_at  \\\n",
       "0  https://www.reddit.com/r/ADHD/comments/1et3kj0...      ADHD  1.723749e+09   \n",
       "1  https://www.reddit.com/r/ADHD/comments/1gk5ftv...      ADHD  1.730809e+09   \n",
       "2  https://www.reddit.com/r/ADHD/comments/1f0k9en...      ADHD  1.724547e+09   \n",
       "3  https://www.reddit.com/r/ADHD/comments/1g7tbb2...      ADHD  1.729409e+09   \n",
       "4  https://www.reddit.com/r/ADHD/comments/1dy8uqs...      ADHD  1.720445e+09   \n",
       "\n",
       "                                           self_text searchQuery  \n",
       "0  Please go out and get tested if you are still ...        adhd  \n",
       "1  A study was published in the [British Journal ...        adhd  \n",
       "2  We all know the statistics: 20,000 behavioral ...        adhd  \n",
       "3  I laugh so hard when I look back at this memor...        adhd  \n",
       "4  I just got diagnosed, and Iâ€™m 39.  My entire l...        adhd  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import\n",
    "import pandas as pd\n",
    "posts_registered=[]\n",
    "query=db.reddit_posts.find({},{'_id':0})\n",
    "for el in query:\n",
    "    posts_registered.append(el)\n",
    "posts_registered_df=pd.DataFrame(posts_registered)\n",
    "posts_registered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosed with Inattentive ADHD at 31. Explains so many things from my childhood.\n",
      "Please go out and get tested if you are still on the fence. I always assumed ADHD was only hyperactive. A lot of concerns about day dreaming, zoning out and inattentiveness came into play during my consult. I didn't even consider my lack of sleep being tied to ADHD. But now that I have a diagnoses, it explains quite a bit from my past. I wasn't just lazy and disorganized. \n",
      "\n",
      "  \n",
      "Again, please go get tested if you suspect anything.\n"
     ]
    }
   ],
   "source": [
    "print(posts_registered_df.iloc[0]['title'])\n",
    "print(posts_registered_df.iloc[0]['self_text'])\n",
    "title_to_test=posts_registered_df.iloc[0]['title']\n",
    "text_to_test=posts_registered_df.iloc[0]['self_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ysidhom/Documents/5IF/adhd_project/mental_health_disorders_analysis/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ysidhom/Documents/5IF/adhd_project/mental_health_disorders_analysis/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you help me with this question? Please analyze the following and provide insights:\n",
      "Title: Diagnosed with Inattentive ADHD at 31. Explains so many things from my childhood.\n",
      "Text: Please go out and get tested if you are still on the fence. I always assumed ADHD was only hyperactive. A lot of concerns about day dreaming, zoning out and inattentiveness came into play during my consult. I didn't even consider my lack of sleep being tied to ADHD. But now that I have a diagnoses, it explains quite a bit from my past. I wasn't just lazy and disorganized. \n",
      "\n",
      "  \n",
      "Again, please go get tested if you suspect anything.\n",
      "\n",
      "Provide an insightful and concise analysis below:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model (already saved locally)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Llama-3.2-1B\")\n",
    "\n",
    "# Set pad token ID to eos token ID if pad token ID is not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Test the model\n",
    "input_text = f\"\"\"Can you help me with this question? Please analyze the following and provide insights:\n",
    "Title: {title_to_test}\n",
    "Text: {text_to_test}\n",
    "\n",
    "Provide an insightful and concise analysis below:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = input_ids.ne(tokenizer.pad_token_id).long()\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,  # Length of the response\n",
    "    temperature=0.7,  # Balance between random and deterministic\n",
    "    top_p=0.9,        # Focus on top 90% of likely tokens\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
