{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleanning and Tokenization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's first try to check for common misspelling of the words in the text and then proceed to clean it\n",
    "#from spellchecker import SpellChecker\n",
    "\n",
    "def correct_spelling(text):\n",
    "    \"\"\"\n",
    "    Correct common misspellings in a text using pyspellchecker.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to correct.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with corrected spelling.\n",
    "        \n",
    "    \"\"\"\n",
    "    spell = SpellChecker()\n",
    "    words = text.split()\n",
    "    corrected_words = [spell.correction(word) if word in spell else word for word in words]\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "        #First let's define the cleanning function\n",
    "    import re\n",
    "    import string\n",
    "    import nltk\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    \n",
    "    #text=correct_spelling(text)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Keep numbers\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    text = \" \".join([stemmer.stem(word) for word in text.split()])     \n",
    "    return text\n",
    "\n",
    "def contains_keywords(text, keywords):\n",
    "    \"\"\"\n",
    "    Check if a given text contains at least one keyword from a list after correcting spelling.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to check.\n",
    "        keywords (list): A list of keywords to look for.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if at least one keyword is found, False otherwise.\n",
    "    \"\"\"\n",
    "    # Correct spelling in the text\n",
    "    corrected_text = correct_spelling(text)\n",
    "    \n",
    "    # Convert corrected text to lowercase for case-insensitive matching\n",
    "    corrected_text = corrected_text.lower()\n",
    "    \n",
    "    # Check if any keyword is in the corrected text\n",
    "    for word in keywords:\n",
    "        if word.lower() in corrected_text:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your input prompt\n",
    "prompt = '''\n",
    "\n",
    "32F Diagnosed with Inattentive ADHD at 31. Explains so many things from my childhood. Please go out and get tested if you are still on the fence. I always assumed ADHD was only hyperactive. A lot of concerns about day dreaming, zoning out and inattentiveness came into play during my consult. I didn't even consider my lack of sleep being tied to ADHD. But now that I have a diagnosis, it explains quite a bit from my past. I wasn't just lazy and disorganized. Again, please go get tested if you suspect anything.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32f diagnos inattent adhd 31 explain mani thing childhood pleas go get test still fenc alway assum adhd hyperact lot concern day dream zone inattent came play consult even consid lack sleep tie adhd diagnosi explain quit bit past lazi disorgan pleas go get test suspect anyth\n"
     ]
    }
   ],
   "source": [
    "print(clean_text(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ysidhom/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the stopwords resource\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
